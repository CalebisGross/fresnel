#!/usr/bin/env python3
"""
Dataset loader for TRELLIS distillation data.

Loads intermediate representations generated by generate_trellis_data.py
for training Fresnel v2's direct decoder.

Data format per sample:
- features.pt: DINOv2 features (1, num_patches, feature_dim)
- coords.pt: Sparse structure coords (N, 4) [batch, x, y, z]
- slat.pt: Structured latent {'feats': (N, C), 'coords': (N, 4)}
- gaussians.ply: TRELLIS PLY format (correct format from TRELLIS save_ply)
"""

import json
from pathlib import Path
from typing import Optional, Tuple, Dict, List
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from plyfile import PlyData


class TrellisDistillationDataset(Dataset):
    """
    Dataset for TRELLIS distillation training.

    Loads pre-computed TRELLIS outputs for training direct decoders.
    """

    def __init__(
        self,
        data_dir: Path,
        max_gaussians: int = 50000,
        max_coords: int = 4000,  # Reduced from 10000 for memory efficiency
        return_mesh: bool = False,
        transform=None,
        occupancy_threshold: float = 0.15,  # Distance threshold for occupancy target (~5 voxels in [-1,1] space)
    ):
        """
        Args:
            data_dir: Directory containing TRELLIS output samples
            max_gaussians: Maximum number of Gaussians to load per sample (for batching)
            max_coords: Maximum number of sparse coords to load per sample
            return_mesh: Whether to load mesh data if available
            transform: Optional transform to apply to features
            occupancy_threshold: Distance threshold for computing occupancy targets.
                A voxel is "occupied" if any target Gaussian is within this distance.
                In [-1,1] space: 0.05 = ~1.6 voxels (too tight), 0.15 = ~5 voxels (balanced),
                0.3 = ~10 voxels (loose). Default 0.15 aims for ~30-50% occupancy.
        """
        self.data_dir = Path(data_dir)
        self.max_gaussians = max_gaussians
        self.max_coords = max_coords
        self.return_mesh = return_mesh
        self.transform = transform
        self.occupancy_threshold = occupancy_threshold

        # Find all valid samples
        self.samples = self._find_samples()
        print(f"Found {len(self.samples)} valid samples in {data_dir}")

    def _find_samples(self) -> List[Path]:
        """Find all valid sample directories."""
        samples = []
        for sample_dir in self.data_dir.iterdir():
            if not sample_dir.is_dir():
                continue

            # Check required files exist (use PLY for correct Gaussian format)
            required = ['features.pt', 'coords.pt', 'slat.pt', 'gaussians.ply']
            if all((sample_dir / f).exists() for f in required):
                samples.append(sample_dir)

        return sorted(samples)

    def _compute_occupancy(
        self,
        coords: torch.Tensor,
        gaussian_positions: torch.Tensor,
        n_coords: int,
        n_gaussians: int,
    ) -> torch.Tensor:
        """
        Compute binary occupancy target based on proximity to target Gaussians.

        A voxel is "occupied" if any target Gaussian is within threshold distance.

        Args:
            coords: Voxel coordinates (max_coords, 4) with [batch_idx, x, y, z]
            gaussian_positions: Target Gaussian positions (max_gaussians, 3) in [-1, 1]
            n_coords: Number of valid coordinates
            n_gaussians: Number of valid Gaussians

        Returns:
            occupancy: (max_coords,) binary occupancy target
        """
        occupancy = torch.zeros(self.max_coords, dtype=torch.float32)

        if n_coords == 0 or n_gaussians == 0:
            return occupancy

        # Get valid coords and Gaussians only
        valid_coords = coords[:n_coords, 1:4].float()  # (n_coords, 3) xyz only
        valid_gaussians = gaussian_positions[:n_gaussians]  # (n_gaussians, 3)

        # Normalize coords to [-1, 1] to match Gaussian positions
        # coords are in [0, 63], normalize to [-1, 1]
        valid_coords_norm = valid_coords / 64.0 * 2 - 1  # (n_coords, 3)

        # Compute pairwise distances using broadcasting
        # valid_coords_norm: (n_coords, 1, 3)
        # valid_gaussians: (1, n_gaussians, 3)
        # dist: (n_coords, n_gaussians)
        dist = torch.cdist(valid_coords_norm.unsqueeze(0), valid_gaussians.unsqueeze(0)).squeeze(0)

        # Voxel is occupied if min distance to any target < threshold
        min_dist = dist.min(dim=1).values  # (n_coords,)
        occupancy[:n_coords] = (min_dist < self.occupancy_threshold).float()

        return occupancy

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        sample_dir = self.samples[idx]

        # Load DINOv2 features (weights_only=True is safe for tensors)
        features = torch.load(sample_dir / "features.pt", map_location='cpu', weights_only=True)
        if features.dim() == 3:
            features = features.squeeze(0)  # Remove batch dim: (num_patches, feature_dim)

        # Load sparse structure coordinates
        coords = torch.load(sample_dir / "coords.pt", map_location='cpu', weights_only=True)

        # Load structured latent (contains dict, need weights_only=False)
        slat_data = torch.load(sample_dir / "slat.pt", map_location='cpu', weights_only=False)
        slat_feats = slat_data['feats']
        slat_coords = slat_data['coords']

        # Load Gaussians from PLY (TRELLIS save_ply format - correct values)
        # PLY format: pos(3), normals(3), f_dc(3), opacity(1), scale(3), rotation(4)
        ply = PlyData.read(sample_dir / "gaussians.ply")
        v = ply['vertex']
        n_gaussians_raw = len(v['x'])

        gaussians = torch.zeros(n_gaussians_raw, 14, dtype=torch.float32)

        # Position: world coords from TRELLIS (already transformed by aabb)
        # Normalize to [-1, 1] based on typical TRELLIS output range
        pos = np.stack([v['x'], v['y'], v['z']], axis=1)
        # TRELLIS outputs in roughly [-0.5, 0.5] range, scale to [-1, 1]
        gaussians[:, 0:3] = torch.from_numpy(pos * 2.0)

        # Scale: log(actual_scale) in PLY, convert to actual scale for training
        # exp to get actual scale, then clamp to reasonable range
        scale = np.stack([v['scale_0'], v['scale_1'], v['scale_2']], axis=1)
        gaussians[:, 3:6] = torch.from_numpy(np.exp(scale)).clamp(1e-6, 1.0)

        # Rotation: quaternion (already normalized in PLY)
        rot = np.stack([v['rot_0'], v['rot_1'], v['rot_2'], v['rot_3']], axis=1)
        quat = torch.from_numpy(rot)
        quat_norm = quat.norm(dim=-1, keepdim=True).clamp(min=1e-6)
        gaussians[:, 6:10] = quat / quat_norm

        # Color: SH DC coefficients - convert to [0, 1] RGB
        # SH DC to RGB: color = SH * C0 + 0.5 where C0 = 0.28209479177387814
        C0 = 0.28209479177387814
        f_dc = np.stack([v['f_dc_0'], v['f_dc_1'], v['f_dc_2']], axis=1)
        color = f_dc * C0 + 0.5
        gaussians[:, 10:13] = torch.from_numpy(color).clamp(0, 1)

        # Opacity: inverse_sigmoid in PLY, convert to [0, 1] via sigmoid
        opacity = torch.sigmoid(torch.from_numpy(np.array(v['opacity'])))
        gaussians[:, 13] = opacity

        # Check for NaN/Inf
        if torch.isnan(gaussians).any() or torch.isinf(gaussians).any():
            print(f"Warning: NaN/Inf in gaussians for {sample_dir.name}, replacing")
            gaussians = torch.nan_to_num(gaussians, nan=0.0, posinf=1.0, neginf=-1.0)

        if torch.isnan(features).any() or torch.isinf(features).any():
            print(f"Warning: NaN/Inf in features for {sample_dir.name}")
            features = torch.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)

        # Pad/truncate to fixed size for batching
        n_gaussians = gaussians.shape[0]
        if n_gaussians > self.max_gaussians:
            # Random subsample
            indices = torch.randperm(n_gaussians)[:self.max_gaussians]
            gaussians = gaussians[indices]
            n_gaussians = self.max_gaussians
        elif n_gaussians < self.max_gaussians:
            # Pad with zeros
            padding = torch.zeros(self.max_gaussians - n_gaussians, 14)
            gaussians = torch.cat([gaussians, padding], dim=0)

        n_coords = coords.shape[0]
        if n_coords > self.max_coords:
            indices = torch.randperm(n_coords)[:self.max_coords]
            coords = coords[indices]
            slat_feats = slat_feats[indices]
            slat_coords = slat_coords[indices]
            n_coords = self.max_coords
        elif n_coords < self.max_coords:
            # Pad with zeros
            coord_padding = torch.zeros(self.max_coords - n_coords, 4, dtype=coords.dtype)
            coords = torch.cat([coords, coord_padding], dim=0)
            feat_padding = torch.zeros(self.max_coords - n_coords, slat_feats.shape[1])
            slat_feats = torch.cat([slat_feats, feat_padding], dim=0)
            slat_coord_padding = torch.zeros(self.max_coords - n_coords, 4, dtype=slat_coords.dtype)
            slat_coords = torch.cat([slat_coords, slat_coord_padding], dim=0)

        # Apply transform if provided
        if self.transform:
            features = self.transform(features)

        # Create mask for valid entries
        gaussian_mask = torch.zeros(self.max_gaussians, dtype=torch.bool)
        gaussian_mask[:n_gaussians] = True

        coord_mask = torch.zeros(self.max_coords, dtype=torch.bool)
        coord_mask[:n_coords] = True

        # Compute occupancy target: which voxels are near target Gaussians
        gaussian_positions = gaussians[:, :3]  # (max_gaussians, 3) positions in [-1, 1]
        occupancy_target = self._compute_occupancy(
            coords, gaussian_positions, n_coords, n_gaussians
        )

        result = {
            'features': features,           # (num_patches, feature_dim)
            'coords': coords,               # (max_coords, 4)
            'coord_mask': coord_mask,       # (max_coords,)
            'slat_feats': slat_feats,       # (max_coords, slat_dim)
            'slat_coords': slat_coords,     # (max_coords, 4)
            'gaussians': gaussians,         # (max_gaussians, 14)
            'gaussian_mask': gaussian_mask, # (max_gaussians,)
            'occupancy_target': occupancy_target,  # (max_coords,) binary occupancy
            'n_gaussians': torch.tensor(n_gaussians),
            'n_coords': torch.tensor(n_coords),
            'sample_name': sample_dir.name,
        }

        return result


class GaussianTargetDataset(Dataset):
    """
    Simplified dataset that only loads features and target Gaussians.

    Use this for direct decoder training (skipping structure prediction).
    """

    def __init__(
        self,
        data_dir: Path,
        max_gaussians: int = 50000,
        feature_dim: int = 1024,
    ):
        self.data_dir = Path(data_dir)
        self.max_gaussians = max_gaussians
        self.feature_dim = feature_dim

        self.samples = self._find_samples()
        print(f"Found {len(self.samples)} samples")

    def _find_samples(self) -> List[Path]:
        samples = []
        for sample_dir in self.data_dir.iterdir():
            if sample_dir.is_dir() and (sample_dir / "gaussians.bin").exists():
                samples.append(sample_dir)
        return sorted(samples)

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        sample_dir = self.samples[idx]

        # Load features
        features = torch.load(sample_dir / "features.pt", map_location='cpu', weights_only=True)
        if features.dim() == 3:
            features = features.squeeze(0)

        # Load Gaussians
        gaussians = np.fromfile(sample_dir / "gaussians.bin", dtype=np.float32)
        gaussians = gaussians.reshape(-1, 14)

        # Check for NaN/Inf
        if np.isnan(gaussians).any() or np.isinf(gaussians).any():
            gaussians = np.nan_to_num(gaussians, nan=0.0, posinf=1.0, neginf=-1.0)

        n_valid = min(len(gaussians), self.max_gaussians)

        # Pad to fixed size
        padded = np.zeros((self.max_gaussians, 14), dtype=np.float32)
        padded[:n_valid] = gaussians[:n_valid]

        gaussians = torch.from_numpy(padded)
        mask = torch.zeros(self.max_gaussians, dtype=torch.bool)
        mask[:n_valid] = True

        # Clean features
        if torch.isnan(features).any() or torch.isinf(features).any():
            features = torch.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)

        return features, gaussians, mask


def create_dataloaders(
    data_dir: Path,
    batch_size: int = 4,
    num_workers: int = 4,
    val_split: float = 0.1,
    max_gaussians: int = 50000,
    max_coords: int = 4000,  # Reduced from 10000 for memory efficiency
) -> Tuple[DataLoader, DataLoader]:
    """
    Create train and validation dataloaders.

    Args:
        data_dir: Directory containing TRELLIS output samples
        batch_size: Batch size for training
        num_workers: Number of data loading workers
        val_split: Fraction of data to use for validation
        max_gaussians: Max Gaussians per sample
        max_coords: Max sparse coords per sample

    Returns:
        (train_loader, val_loader)
    """
    dataset = TrellisDistillationDataset(
        data_dir,
        max_gaussians=max_gaussians,
        max_coords=max_coords,
    )

    # Split into train/val
    n_val = int(len(dataset) * val_split)
    n_train = len(dataset) - n_val

    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [n_train, n_val],
        generator=torch.Generator().manual_seed(42)
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )

    return train_loader, val_loader


def collate_variable_gaussians(batch: List[Dict]) -> Dict[str, torch.Tensor]:
    """
    Custom collate function for variable-length Gaussian outputs.

    Handles samples with different numbers of Gaussians/coords.
    """
    # Stack fixed-size tensors
    features = torch.stack([b['features'] for b in batch])
    coords = torch.stack([b['coords'] for b in batch])
    coord_mask = torch.stack([b['coord_mask'] for b in batch])
    slat_feats = torch.stack([b['slat_feats'] for b in batch])
    slat_coords = torch.stack([b['slat_coords'] for b in batch])
    gaussians = torch.stack([b['gaussians'] for b in batch])
    gaussian_mask = torch.stack([b['gaussian_mask'] for b in batch])
    occupancy_target = torch.stack([b['occupancy_target'] for b in batch])
    n_gaussians = torch.stack([b['n_gaussians'] for b in batch])
    n_coords = torch.stack([b['n_coords'] for b in batch])

    return {
        'features': features,
        'coords': coords,
        'coord_mask': coord_mask,
        'slat_feats': slat_feats,
        'slat_coords': slat_coords,
        'gaussians': gaussians,
        'gaussian_mask': gaussian_mask,
        'occupancy_target': occupancy_target,
        'n_gaussians': n_gaussians,
        'n_coords': n_coords,
        'sample_names': [b['sample_name'] for b in batch],
    }


if __name__ == "__main__":
    # Test the dataset
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=Path, required=True)
    parser.add_argument("--batch_size", type=int, default=2)
    args = parser.parse_args()

    train_loader, val_loader = create_dataloaders(
        args.data_dir,
        batch_size=args.batch_size,
        num_workers=0,  # For debugging
    )

    print(f"Train batches: {len(train_loader)}")
    print(f"Val batches: {len(val_loader)}")

    # Test one batch
    batch = next(iter(train_loader))
    print("\nBatch contents:")
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape} {value.dtype}")
        else:
            print(f"  {key}: {type(value)}")
