#!/usr/bin/env python3
"""
Generate distillation dataset using TRELLIS as teacher.

This script calls TRELLIS-AMD via subprocess using its own virtual environment.
The pipeline is loaded ONCE and processes all images in a single run.

Outputs per image:
- image.png: Preprocessed input image (518x518)
- features.pt: DINOv2 features
- coords.pt: Sparse structure occupancy coordinates
- slat.pt: Structured latent features
- gaussians.ply: Final Gaussian output in PLY format
- gaussians.bin: Fresnel-format binary (14 floats per Gaussian)

Usage:
    python generate_trellis_data.py --input_dir /path/to/images --output_dir /path/to/output
    python generate_trellis_data.py --input_dir images/training --output_dir data/trellis_distillation --resume
"""

import os
import sys
import argparse
import subprocess
import json
from pathlib import Path
from typing import List
import shutil

# Paths
FRESNEL_DIR = Path(__file__).parent.parent.parent
TRELLIS_DIR = FRESNEL_DIR.parent / "TRELLIS-AMD"
TRELLIS_VENV = TRELLIS_DIR / ".venv"
TRELLIS_PYTHON = TRELLIS_VENV / "bin" / "python"


def check_trellis_setup():
    """Verify TRELLIS-AMD is properly set up."""
    if not TRELLIS_DIR.exists():
        print(f"ERROR: TRELLIS-AMD not found at {TRELLIS_DIR}")
        sys.exit(1)

    if not TRELLIS_VENV.exists():
        print(f"ERROR: TRELLIS-AMD venv not found at {TRELLIS_VENV}")
        print("Please run: cd TRELLIS-AMD && ./install_amd.sh")
        sys.exit(1)

    if not TRELLIS_PYTHON.exists():
        print(f"ERROR: TRELLIS Python not found at {TRELLIS_PYTHON}")
        sys.exit(1)

    print(f"TRELLIS-AMD found at: {TRELLIS_DIR}")
    print(f"Using Python: {TRELLIS_PYTHON}")


def get_image_paths(input_dir: Path, num_samples: int) -> List[Path]:
    """Get list of image paths to process."""
    extensions = {'.png', '.jpg', '.jpeg', '.webp'}
    paths = [p for p in input_dir.iterdir() if p.suffix.lower() in extensions]
    paths = sorted(paths)
    if num_samples > 0:
        paths = paths[:num_samples]
    return paths


def create_trellis_batch_worker_script(
    output_path: Path,
    image_paths: List[Path],
    output_dir: Path,
    num_steps: int,
    seed: int,
    resume: bool,
    batch_size: int,
):
    """Create a Python script that processes ALL images in one run."""

    # Convert paths to strings for embedding in script
    image_paths_str = "[" + ", ".join(f'Path("{p.resolve()}")' for p in image_paths) + "]"

    script = f'''#!/usr/bin/env python3
"""
TRELLIS batch worker script - processes all images in one run.
Generated by Fresnel's generate_trellis_data.py
"""
import os
import sys
import gc
import json
import traceback
from pathlib import Path

# Set environment BEFORE any imports
os.environ["HSA_OVERRIDE_GFX_VERSION"] = "11.0.0"
os.environ["ATTN_BACKEND"] = "sdpa"
os.environ["XFORMERS_DISABLED"] = "1"
os.environ["SPARSE_BACKEND"] = "torchsparse"

import torch
import numpy as np
from PIL import Image

# Add TRELLIS to path
TRELLIS_DIR = Path(__file__).parent
sys.path.insert(0, str(TRELLIS_DIR))

# Configure torchsparse for AMD/ROCm (must be done before importing trellis)
try:
    from torchsparse.nn.functional.conv.conv_config import (
        Dataflow, set_global_conv_config, _default_conv_config
    )
    from torchsparse.nn.functional.conv.conv_mode import set_conv_mode
    hip_config = _default_conv_config.copy()
    hip_config['dataflow'] = Dataflow.GatherScatter
    set_global_conv_config(hip_config)
    set_conv_mode(0)
    print("Configured torchsparse for AMD/ROCm")
except Exception as e:
    print(f"Warning: Could not configure torchsparse: {{e}}")

from trellis.pipelines import TrellisImageTo3DPipeline


def save_gaussian_ply(gaussians, output_path: Path):
    """Save Gaussians to PLY format."""
    if hasattr(gaussians, 'save_ply'):
        gaussians.save_ply(str(output_path))
    else:
        from plyfile import PlyData, PlyElement

        xyz = gaussians._xyz.detach().cpu().numpy()
        normals = np.zeros_like(xyz)
        f_dc = gaussians._features_dc.detach().cpu().numpy().reshape(-1, 3)
        scale = gaussians._scaling.detach().cpu().numpy()
        rotation = gaussians._rotation.detach().cpu().numpy()
        opacity = gaussians._opacity.detach().cpu().numpy()

        dtype = [
            ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),
            ('nx', 'f4'), ('ny', 'f4'), ('nz', 'f4'),
            ('f_dc_0', 'f4'), ('f_dc_1', 'f4'), ('f_dc_2', 'f4'),
            ('opacity', 'f4'),
            ('scale_0', 'f4'), ('scale_1', 'f4'), ('scale_2', 'f4'),
            ('rot_0', 'f4'), ('rot_1', 'f4'), ('rot_2', 'f4'), ('rot_3', 'f4'),
        ]

        elements = np.empty(xyz.shape[0], dtype=dtype)
        elements['x'] = xyz[:, 0]
        elements['y'] = xyz[:, 1]
        elements['z'] = xyz[:, 2]
        elements['nx'] = normals[:, 0]
        elements['ny'] = normals[:, 1]
        elements['nz'] = normals[:, 2]
        elements['f_dc_0'] = f_dc[:, 0]
        elements['f_dc_1'] = f_dc[:, 1]
        elements['f_dc_2'] = f_dc[:, 2]
        elements['opacity'] = opacity.squeeze()
        elements['scale_0'] = scale[:, 0]
        elements['scale_1'] = scale[:, 1]
        elements['scale_2'] = scale[:, 2]
        elements['rot_0'] = rotation[:, 0]
        elements['rot_1'] = rotation[:, 1]
        elements['rot_2'] = rotation[:, 2]
        elements['rot_3'] = rotation[:, 3]

        el = PlyElement.describe(elements, 'vertex')
        PlyData([el]).write(str(output_path))


@torch.no_grad()
def process_image(pipeline, image_path: Path, output_dir: Path, seed: int, num_steps: int):
    """Process a single image and save all intermediate representations."""

    sample_dir = output_dir / image_path.stem
    sample_dir.mkdir(parents=True, exist_ok=True)

    # Load and preprocess image
    image = Image.open(image_path).convert('RGBA')
    preprocessed = pipeline.preprocess_image(image)
    preprocessed.save(sample_dir / "image.png")
    del image

    # Get conditioning (DINOv2 features)
    cond = pipeline.get_cond([preprocessed])
    features = cond['cond']
    torch.save(features.cpu(), sample_dir / "features.pt")
    del preprocessed

    # Sample sparse structure (Stage 1)
    torch.manual_seed(seed)
    sampler_params = {{'steps': num_steps}}
    coords = pipeline.sample_sparse_structure(cond, num_samples=1, sampler_params=sampler_params)
    torch.save(coords.cpu(), sample_dir / "coords.pt")

    # Sample structured latent (Stage 2)
    slat = pipeline.sample_slat(cond, coords, sampler_params=sampler_params)
    del cond  # Free conditioning tensors

    # Save SLat
    slat_data = {{
        'feats': slat.feats.cpu(),
        'coords': slat.coords.cpu(),
    }}
    torch.save(slat_data, sample_dir / "slat.pt")

    # Decode to Gaussians
    outputs = pipeline.decode_slat(slat, formats=['gaussian'])
    del slat  # Free SLat tensors
    gaussians = outputs['gaussian'][0] if isinstance(outputs['gaussian'], list) else outputs['gaussian']
    save_gaussian_ply(gaussians, sample_dir / "gaussians.ply")

    # Save as Fresnel binary format (14 floats per Gaussian)
    n_gaussians = gaussians._xyz.shape[0]
    n_voxels = int(coords.shape[0])
    feat_shape = list(features.shape)
    slat_shape = list(slat_data['feats'].shape)

    # Save ACTUAL Gaussian values (after TRELLIS's activation functions)
    # NOT raw internal representations - those require complex transforms
    fresnel_data = np.zeros((n_gaussians, 14), dtype=np.float32)

    # Position: get_xyz applies aabb transform to get world coordinates
    fresnel_data[:, 0:3] = gaussians.get_xyz.detach().cpu().numpy()

    # Scale: get_scaling applies exp + bias + min_kernel, then log for storage
    # This matches what TRELLIS save_ply does
    fresnel_data[:, 3:6] = torch.log(gaussians.get_scaling).detach().cpu().numpy()

    # Rotation: get_rotation applies normalization and bias
    fresnel_data[:, 6:10] = gaussians.get_rotation.detach().cpu().numpy()

    # Color: SH DC coefficients (keep as-is)
    fresnel_data[:, 10:13] = gaussians._features_dc.detach().cpu().numpy().reshape(-1, 3)

    # Opacity: get_opacity applies sigmoid, then inverse_sigmoid for storage
    # This matches what TRELLIS save_ply does
    from trellis.representations.gaussian.general_utils import inverse_sigmoid
    fresnel_data[:, 13] = inverse_sigmoid(gaussians.get_opacity).detach().cpu().numpy().squeeze()

    fresnel_data.tofile(sample_dir / "gaussians.bin")

    del gaussians, outputs, features, coords, slat_data

    # Save metadata
    metadata = {{
        'source_image': str(image_path),
        'seed': seed,
        'num_steps': num_steps,
        'num_gaussians': n_gaussians,
        'num_occupied_voxels': n_voxels,
        'feature_shape': feat_shape,
        'slat_shape': slat_shape,
    }}
    with open(sample_dir / "metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)

    return metadata


def load_failure_log(output_dir: Path) -> dict:
    """Load failure counts from log file."""
    log_path = output_dir / "_failures.json"
    if log_path.exists():
        with open(log_path, 'r') as f:
            return json.load(f)
    return {{}}


def save_failure_log(output_dir: Path, failures: dict):
    """Save failure counts to log file."""
    log_path = output_dir / "_failures.json"
    with open(log_path, 'w') as f:
        json.dump(failures, f, indent=2)


def main():
    # Configuration
    image_paths = {image_paths_str}
    output_dir = Path("{output_dir.resolve()}")
    num_steps = {num_steps}
    seed = {seed}
    resume = {resume}
    batch_size = {batch_size}  # Process this many then exit for clean restart
    max_failures = 2  # Skip images after this many failures

    output_dir.mkdir(parents=True, exist_ok=True)

    # Load failure tracking
    failures = load_failure_log(output_dir)

    print(f"Loading TRELLIS pipeline...")
    pipeline = TrellisImageTo3DPipeline.from_pretrained("JeffreyXiang/TRELLIS-image-large")
    pipeline.cuda()
    print("Pipeline loaded")

    success_count = 0
    fail_count = 0
    skip_count = 0
    processed_this_run = 0
    total = len(image_paths)

    for i, image_path in enumerate(image_paths):
        image_name = image_path.name
        sample_dir = output_dir / image_path.stem

        # Check if already processed (resume mode)
        if resume and (sample_dir / "gaussians.ply").exists():
            continue  # Skip silently

        # Check if failed too many times
        if failures.get(image_name, 0) >= max_failures:
            skip_count += 1
            continue  # Skip permanently failed images

        print(f"\\n[{{i+1}}/{{total}}] Processing {{image_name}}...")

        try:
            metadata = process_image(pipeline, image_path, output_dir, seed, num_steps)
            print(f"  Done: {{metadata['num_gaussians']}} gaussians, {{metadata['num_occupied_voxels']}} voxels")
            success_count += 1
            processed_this_run += 1
            # Clear from failures if it succeeded
            if image_name in failures:
                del failures[image_name]
                save_failure_log(output_dir, failures)
        except Exception as e:
            print(f"  ERROR: {{e}}")
            traceback.print_exc()
            fail_count += 1
            processed_this_run += 1
            # Track failure
            failures[image_name] = failures.get(image_name, 0) + 1
            save_failure_log(output_dir, failures)
            print(f"  Failure count for {{image_name}}: {{failures[image_name]}}/{{max_failures}}")

        # Aggressive memory cleanup after each image
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()

        # Exit after batch_size images to allow clean restart
        if batch_size > 0 and processed_this_run >= batch_size:
            print(f"\\n" + "="*60)
            print(f"Batch of {{batch_size}} complete. Exiting for clean restart.")
            print(f"  Processed this run: {{processed_this_run}}")
            print(f"  Skipped (max failures): {{skip_count}}")
            print(f"  Run again with --resume to continue.")
            print("="*60)
            return

    print(f"\\n" + "="*60)
    print(f"All processing complete!")
    print(f"  Success: {{success_count}}")
    print(f"  Failed: {{fail_count}}")
    print(f"  Skipped (max failures): {{skip_count}}")
    print("="*60)


if __name__ == "__main__":
    main()
'''
    output_path.write_text(script)
    output_path.chmod(0o755)


def main():
    parser = argparse.ArgumentParser(description="Generate TRELLIS distillation dataset")
    parser.add_argument("--input_dir", type=Path, required=True,
                        help="Directory containing input images")
    parser.add_argument("--output_dir", type=Path, default=Path("data/trellis_distillation"),
                        help="Output directory for generated data")
    parser.add_argument("--num_samples", type=int, default=0,
                        help="Max samples to process (0 = all)")
    parser.add_argument("--num_steps", type=int, default=12,
                        help="Diffusion steps for TRELLIS")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed")
    parser.add_argument("--resume", action="store_true",
                        help="Skip already processed images")
    parser.add_argument("--batch_size", type=int, default=50,
                        help="Process this many images then exit (for memory cleanup). 0=no limit")

    args = parser.parse_args()

    # Check TRELLIS setup
    check_trellis_setup()

    # Get image paths
    image_paths = get_image_paths(args.input_dir, args.num_samples)
    print(f"Found {len(image_paths)} images to process")

    if len(image_paths) == 0:
        print("No images found!")
        return

    # Create output directory
    args.output_dir.mkdir(parents=True, exist_ok=True)

    # Create batch worker script
    worker_script = TRELLIS_DIR / "_fresnel_batch_worker.py"
    create_trellis_batch_worker_script(
        worker_script,
        image_paths,
        args.output_dir,
        args.num_steps,
        args.seed,
        args.resume,
        args.batch_size,
    )
    print(f"Created batch worker script: {worker_script}")

    # Set environment
    env = os.environ.copy()
    env["HSA_OVERRIDE_GFX_VERSION"] = "11.0.0"
    env["ATTN_BACKEND"] = "sdpa"
    env["XFORMERS_DISABLED"] = "1"
    env["SPARSE_BACKEND"] = "torchsparse"

    # Run the batch worker
    cmd = [str(TRELLIS_PYTHON), str(worker_script)]

    print(f"\nStarting TRELLIS batch processing...")
    print(f"{'='*60}")

    try:
        result = subprocess.run(
            cmd,
            cwd=str(TRELLIS_DIR),
            env=env,
        )

        if result.returncode != 0:
            print(f"Worker exited with code {result.returncode}")

    except KeyboardInterrupt:
        print("\nInterrupted by user")
    finally:
        # Cleanup worker script
        if worker_script.exists():
            worker_script.unlink()
            print(f"Cleaned up worker script")


if __name__ == "__main__":
    main()
