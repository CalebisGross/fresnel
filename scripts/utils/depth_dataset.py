"""
Depth Dataset Loaders - For training tiny depth estimation models.

Supports:
1. NYU Depth V2 (via HuggingFace - easy download)
2. Custom folder with images (pseudo-labels generated by DA V2)
3. Synthetic depth (for testing/debugging)

Learning concepts:
- How depth datasets are structured
- Data augmentation for depth estimation
- Why scale-invariant training matters
"""

import os
import random
from pathlib import Path
from typing import Optional, Callable, Tuple

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import torchvision.transforms as T
import torchvision.transforms.functional as TF


class DepthAugmentation:
    """
    Data augmentation for depth estimation.

    Key insight: We need to apply the SAME spatial transform to both
    RGB and depth, but color augmentations only apply to RGB.
    """

    def __init__(
        self,
        size: Tuple[int, int] = (256, 256),
        horizontal_flip: bool = True,
        color_jitter: bool = True,
        random_crop: bool = True,
    ):
        self.size = size
        self.horizontal_flip = horizontal_flip
        self.color_jitter = color_jitter
        self.random_crop = random_crop

        # Color augmentation (RGB only)
        if color_jitter:
            self.color_transform = T.ColorJitter(
                brightness=0.2,
                contrast=0.2,
                saturation=0.2,
                hue=0.1
            )

    def __call__(
        self,
        rgb: Image.Image,
        depth: np.ndarray
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Apply augmentations to RGB and depth pair.

        Args:
            rgb: PIL RGB image
            depth: numpy array of depth values

        Returns:
            rgb_tensor: (3, H, W) tensor, normalized to [0, 1]
            depth_tensor: (1, H, W) tensor, normalized to [0, 1]
        """
        # Convert depth to PIL for consistent transforms
        # Normalize to 0-255 for PIL
        depth_normalized = (depth - depth.min()) / (depth.max() - depth.min() + 1e-8)
        depth_pil = Image.fromarray((depth_normalized * 255).astype(np.uint8))

        # Random crop (same for both)
        if self.random_crop and random.random() > 0.5:
            # Crop to random 80-100% of image, then resize
            scale = random.uniform(0.8, 1.0)
            crop_h = int(rgb.height * scale)
            crop_w = int(rgb.width * scale)
            top = random.randint(0, rgb.height - crop_h)
            left = random.randint(0, rgb.width - crop_w)

            rgb = TF.crop(rgb, top, left, crop_h, crop_w)
            depth_pil = TF.crop(depth_pil, top, left, crop_h, crop_w)

        # Resize to target size
        rgb = TF.resize(rgb, self.size, interpolation=TF.InterpolationMode.BILINEAR)
        depth_pil = TF.resize(depth_pil, self.size, interpolation=TF.InterpolationMode.NEAREST)

        # Horizontal flip (same for both)
        if self.horizontal_flip and random.random() > 0.5:
            rgb = TF.hflip(rgb)
            depth_pil = TF.hflip(depth_pil)

        # Color jitter (RGB only)
        if self.color_jitter:
            rgb = self.color_transform(rgb)

        # Convert to tensors
        rgb_tensor = TF.to_tensor(rgb)  # (3, H, W), [0, 1]
        depth_tensor = TF.to_tensor(depth_pil)  # (1, H, W), [0, 1]

        return rgb_tensor, depth_tensor


class NYUDepthV2Dataset(Dataset):
    """
    NYU Depth V2 dataset via HuggingFace.

    This is the standard benchmark for indoor depth estimation.
    Uses tanganke/nyuv2 which is in parquet format (not script-based).
    795 train / 654 val samples at 288x384 resolution.

    Usage:
        dataset = NYUDepthV2Dataset(split='train')
        rgb, depth = dataset[0]
    """

    def __init__(
        self,
        split: str = 'train',
        size: Tuple[int, int] = (256, 256),
        augment: bool = True,
        max_samples: Optional[int] = None,
    ):
        """
        Args:
            split: 'train' or 'validation' (mapped to 'val')
            size: Output image size (H, W)
            augment: Apply data augmentation
            max_samples: Limit number of samples (for quick experiments)
        """
        from datasets import load_dataset

        # Map 'validation' to 'val' for this dataset
        hf_split = 'val' if split == 'validation' else split

        print(f"Loading NYU Depth V2 from HuggingFace (tanganke/nyuv2)...")
        self.dataset = load_dataset("tanganke/nyuv2", split=hf_split)

        if max_samples:
            self.dataset = self.dataset.select(range(min(max_samples, len(self.dataset))))

        print(f"Loaded {len(self.dataset)} samples for {split}")

        self.size = size
        self.augment = augment

        if augment:
            self.transform = DepthAugmentation(size=size)
        else:
            self.transform = None

        print(f"Loaded {len(self.dataset)} samples")

    def __len__(self) -> int:
        return len(self.dataset)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        sample = self.dataset[idx]

        # tanganke/nyuv2 format:
        # - image: (3, 288, 384) float32 array
        # - depth: (1, 288, 384) float32 array
        rgb_array = np.array(sample['image'])  # (3, H, W)
        depth_array = np.array(sample['depth'])  # (1, H, W)

        # Convert to PIL for augmentation compatibility
        # RGB: (3, H, W) -> (H, W, 3) -> PIL
        rgb_hwc = np.transpose(rgb_array, (1, 2, 0))  # (H, W, 3)
        # Ensure values are in 0-255 range for PIL
        if rgb_hwc.max() <= 1.0:
            rgb_hwc = (rgb_hwc * 255).astype(np.uint8)
        else:
            rgb_hwc = rgb_hwc.astype(np.uint8)
        rgb = Image.fromarray(rgb_hwc)

        # Depth: (1, H, W) -> (H, W)
        depth = depth_array.squeeze(0)  # (H, W)

        if self.transform:
            rgb_tensor, depth_tensor = self.transform(rgb, depth)
        else:
            # Just resize and convert
            rgb = rgb.resize(self.size, Image.BILINEAR)
            rgb_tensor = TF.to_tensor(rgb)

            depth_normalized = (depth - depth.min()) / (depth.max() - depth.min() + 1e-8)
            depth_pil = Image.fromarray((depth_normalized * 255).astype(np.uint8))
            depth_pil = depth_pil.resize(self.size, Image.NEAREST)
            depth_tensor = TF.to_tensor(depth_pil)

        return rgb_tensor, depth_tensor


class FolderDepthDataset(Dataset):
    """
    Custom folder dataset for RGB + depth pairs.

    Folder structure:
        root/
            images/
                001.jpg
                002.jpg
                ...
            depths/
                001.npy  (or .png)
                002.npy
                ...

    Depth can be:
    - .npy: Raw float32 numpy array
    - .png: 16-bit PNG (depth in millimeters)
    - .bin: Raw float32 binary (width x height floats)

    Use generate_pseudo_labels.py to create depths/ from DA V2!
    """

    def __init__(
        self,
        root: str,
        size: Tuple[int, int] = (256, 256),
        augment: bool = True,
        depth_format: str = 'npy',
    ):
        """
        Args:
            root: Root folder containing images/ and depths/
            size: Output image size (H, W)
            augment: Apply data augmentation
            depth_format: 'npy', 'png', or 'bin'
        """
        self.root = Path(root)
        self.images_dir = self.root / 'images'
        self.depths_dir = self.root / 'depths'
        self.size = size
        self.depth_format = depth_format

        # Find all images
        self.image_paths = sorted(list(self.images_dir.glob('*.jpg')) +
                                  list(self.images_dir.glob('*.png')) +
                                  list(self.images_dir.glob('*.jpeg')))

        if len(self.image_paths) == 0:
            raise ValueError(f"No images found in {self.images_dir}")

        # Verify depths exist
        self._verify_depths()

        self.transform = DepthAugmentation(size=size) if augment else None
        print(f"Loaded {len(self.image_paths)} image/depth pairs from {root}")

    def _verify_depths(self):
        """Check that depth files exist for all images"""
        missing = []
        for img_path in self.image_paths:
            depth_path = self._get_depth_path(img_path)
            if not depth_path.exists():
                missing.append(img_path.name)

        if missing:
            raise ValueError(f"Missing depth files for: {missing[:5]}... ({len(missing)} total)")

    def _get_depth_path(self, img_path: Path) -> Path:
        """Get corresponding depth file path"""
        stem = img_path.stem
        return self.depths_dir / f"{stem}.{self.depth_format}"

    def _load_depth(self, path: Path) -> np.ndarray:
        """Load depth from various formats"""
        if self.depth_format == 'npy':
            return np.load(path)
        elif self.depth_format == 'png':
            # 16-bit PNG, depth in mm
            depth_img = Image.open(path)
            return np.array(depth_img).astype(np.float32) / 1000.0  # mm -> m
        elif self.depth_format == 'bin':
            # Raw float32 binary
            return np.fromfile(path, dtype=np.float32)
        else:
            raise ValueError(f"Unknown depth format: {self.depth_format}")

    def __len__(self) -> int:
        return len(self.image_paths)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        img_path = self.image_paths[idx]
        depth_path = self._get_depth_path(img_path)

        # Load RGB
        rgb = Image.open(img_path).convert('RGB')

        # Load depth
        depth = self._load_depth(depth_path)
        if len(depth.shape) == 1:
            # Assume square if 1D
            side = int(np.sqrt(depth.shape[0]))
            depth = depth.reshape(side, side)

        if self.transform:
            rgb_tensor, depth_tensor = self.transform(rgb, depth)
        else:
            rgb = rgb.resize(self.size, Image.BILINEAR)
            rgb_tensor = TF.to_tensor(rgb)

            depth_normalized = (depth - depth.min()) / (depth.max() - depth.min() + 1e-8)
            depth_pil = Image.fromarray((depth_normalized * 255).astype(np.uint8))
            depth_pil = depth_pil.resize(self.size, Image.NEAREST)
            depth_tensor = TF.to_tensor(depth_pil)

        return rgb_tensor, depth_tensor


class SyntheticDepthDataset(Dataset):
    """
    Synthetic dataset for testing/debugging.

    Generates random shapes with known depth - useful for:
    1. Testing the training pipeline without downloading data
    2. Understanding what the network learns
    3. Debugging loss functions
    """

    def __init__(
        self,
        num_samples: int = 1000,
        size: Tuple[int, int] = (256, 256),
    ):
        self.num_samples = num_samples
        self.size = size

    def __len__(self) -> int:
        return self.num_samples

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        H, W = self.size

        # Seed for reproducibility per sample
        rng = np.random.RandomState(idx)

        # Create depth with random shapes
        depth = np.ones((H, W), dtype=np.float32) * 0.8  # Background depth

        # Add random circles at different depths
        num_circles = rng.randint(3, 8)
        for _ in range(num_circles):
            cx, cy = rng.randint(0, W), rng.randint(0, H)
            radius = rng.randint(20, 60)
            circle_depth = rng.uniform(0.2, 0.7)

            y, x = np.ogrid[:H, :W]
            mask = (x - cx) ** 2 + (y - cy) ** 2 <= radius ** 2
            depth[mask] = circle_depth

        # Add smooth gradient for variety
        gradient = np.linspace(0, 0.2, W, dtype=np.float32).reshape(1, -1)
        depth = depth + gradient * rng.uniform(-1, 1)

        # Normalize to [0, 1]
        depth = (depth - depth.min()) / (depth.max() - depth.min() + 1e-8)
        depth = depth.astype(np.float32)

        # Create fake RGB (depth-correlated colors for easy learning)
        rgb = np.zeros((H, W, 3), dtype=np.float32)
        rgb[:, :, 0] = depth  # Red channel = depth
        rgb[:, :, 1] = 1 - depth  # Green = inverse depth
        rgb[:, :, 2] = 0.5  # Blue = constant

        # Add some noise
        rgb = rgb + rng.randn(H, W, 3).astype(np.float32) * 0.1
        rgb = np.clip(rgb, 0, 1).astype(np.float32)

        # Convert to tensors
        rgb_tensor = torch.from_numpy(rgb).permute(2, 0, 1).float()  # (3, H, W)
        depth_tensor = torch.from_numpy(depth).unsqueeze(0).float()  # (1, H, W)

        return rgb_tensor, depth_tensor


def create_dataloader(
    dataset_type: str = 'nyu',
    batch_size: int = 8,
    num_workers: int = 4,
    **kwargs
) -> Tuple[DataLoader, Optional[DataLoader]]:
    """
    Create train and validation dataloaders.

    Args:
        dataset_type: 'nyu', 'folder', or 'synthetic'
        batch_size: Batch size for training
        num_workers: Number of data loading workers
        **kwargs: Additional arguments for dataset

    Returns:
        train_loader, val_loader (val_loader may be None for synthetic)
    """
    if dataset_type == 'nyu':
        train_dataset = NYUDepthV2Dataset(split='train', augment=True, **kwargs)
        val_dataset = NYUDepthV2Dataset(split='validation', augment=False, **kwargs)

    elif dataset_type == 'folder':
        # For folder, use 90/10 split
        full_dataset = FolderDepthDataset(augment=True, **kwargs)
        train_size = int(0.9 * len(full_dataset))
        val_size = len(full_dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size]
        )
        # Disable augment for validation
        val_dataset.dataset.transform = None

    elif dataset_type == 'synthetic':
        train_dataset = SyntheticDepthDataset(num_samples=kwargs.get('num_samples', 1000))
        val_dataset = SyntheticDepthDataset(num_samples=100)

    else:
        raise ValueError(f"Unknown dataset type: {dataset_type}")

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )

    return train_loader, val_loader


if __name__ == '__main__':
    print("=" * 60)
    print("Testing Synthetic Dataset")
    print("=" * 60)

    dataset = SyntheticDepthDataset(num_samples=10)
    rgb, depth = dataset[0]
    print(f"RGB shape: {rgb.shape}, range: [{rgb.min():.3f}, {rgb.max():.3f}]")
    print(f"Depth shape: {depth.shape}, range: [{depth.min():.3f}, {depth.max():.3f}]")

    # Save a sample
    from torchvision.utils import save_image
    save_image(rgb, 'test_rgb.png')
    save_image(depth, 'test_depth.png')
    print("Saved test_rgb.png and test_depth.png")

    print("\n" + "=" * 60)
    print("Testing DataLoader")
    print("=" * 60)

    train_loader, val_loader = create_dataloader(
        dataset_type='synthetic',
        batch_size=4,
        num_workers=0
    )

    for batch_idx, (rgb_batch, depth_batch) in enumerate(train_loader):
        print(f"Batch {batch_idx}: RGB {rgb_batch.shape}, Depth {depth_batch.shape}")
        if batch_idx >= 2:
            break

    print("\nTo use NYU Depth V2, run:")
    print("  train_loader, val_loader = create_dataloader('nyu', batch_size=8)")
    print("\nTo use custom folder:")
    print("  train_loader, val_loader = create_dataloader('folder', root='./data')")
